{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "688c5c00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup environment to load precomputed embeddings\n",
        "import os, sys, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# Add parent directory to sys.path\n",
        "notebook_dir = Path().resolve()\n",
        "project_dir = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
        "sys.path.insert(0, str(project_dir))\n",
        "\n",
        "# Set paths\n",
        "data_dir = project_dir / 'data'\n",
        "embeddings_dir = data_dir / 'embeddings'\n",
        "analysis_dir = data_dir / 'analysis' / 'locus_embeddings'\n",
        "\n",
        "def load_embeddings(locus, class_type):\n",
        "    embeddings_file = analysis_dir / class_type / 'embeddings' / f\"hla_{locus}_embeddings.pkl\"\n",
        "    if not embeddings_file.exists():\n",
        "        print(f\"File {embeddings_file} not found. Run the analysis script first.\")\n",
        "        return None\n",
        "    with open(embeddings_file, 'rb') as f:\n",
        "        embeddings = pickle.load(f)\n",
        "    print(f\"Loaded {len(embeddings)} embeddings for HLA-{locus}\")\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc2dd774",
      "metadata": {},
      "source": [
        "# B Locus Clustering Analysis\n",
        "\n",
        "This notebook encodes the alleles at the B locus, performs clustering on the embeddings, and compares the cluster labels to the serological groupings (e.g. B*01, B*02, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17f5fbd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Attempt to load precomputed embeddings for B locus\n",
        "embeddings = load_embeddings('B', 'class1')\n",
        "if embeddings is None:\n",
        "    # Precomputed embeddings not found; fall back to encoding sequences\n",
        "    allele_file = 'data/raw/fasta/B_prot.fasta'\n",
        "    with open(allele_file, 'r') as f:\n",
        "        sequences = f.read().splitlines()\n",
        "    from src.models.protbert import encode_sequences  # Ensure this function exists\n",
        "    embeddings = encode_sequences(sequences)\n",
        "if embeddings is not None and embeddings:\n",
        "    # Show some basic stats\n",
        "    allele = list(embeddings.keys())[0]\n",
        "    embedding = embeddings[allele]\n",
        "    print(f\"Sample allele: {allele}\")\n",
        "    print(f\"Embedding shape: {embedding.shape}\")\n",
        "    print(f\"First 5 dimensions: {embedding[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "def cluster_embeddings(embeddings, max_clusters=36):\n",
        "    # Convert embeddings to array\n",
        "    X = np.array(list(embeddings.values()))\n",
        "\n",
        "    distortions = []\n",
        "    silhouette_scores = []\n",
        "    calinski_scores = []\n",
        "    davies_scores = []\n",
        "\n",
        "    # Loop over possible number of clusters from 1 to max_clusters\n",
        "    for i in range(1, max_clusters+1):\n",
        "        kmeans = KMeans(n_clusters=i, random_state=0).fit(X)\n",
        "        distortions.append(kmeans.inertia_)\n",
        "        if i > 1:\n",
        "            labels = kmeans.labels_\n",
        "            silhouette = silhouette_score(X, labels)\n",
        "            calinski = calinski_harabasz_score(X, labels)\n",
        "            davies = davies_bouldin_score(X, labels)\n",
        "            silhouette_scores.append(silhouette)\n",
        "            calinski_scores.append(calinski)\n",
        "            davies_scores.append(davies)\n",
        "        else:\n",
        "            # For 1 cluster these metrics are undefined; insert NaN for alignment.\n",
        "            silhouette_scores.append(np.nan)\n",
        "            calinski_scores.append(np.nan)\n",
        "            davies_scores.append(np.nan)\n",
        "\n",
        "    cluster_range = range(1, max_clusters+1)\n",
        "    # Metrics computed for cluster counts from 2 to max_clusters (ignoring cluster==1 for non-inertia metrics)\n",
        "    cluster_range_metrics = range(2, max_clusters+1)\n",
        "\n",
        "    # Plot metrics in a grid of subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    # Elbow Curve (Distortion/Inertia)\n",
        "    axes[0, 0].plot(cluster_range, distortions, marker='o')\n",
        "    axes[0, 0].set_title('Elbow Curve (Distortion)')\n",
        "    axes[0, 0].set_xlabel('Number of clusters')\n",
        "    axes[0, 0].set_ylabel('Distortion (Inertia)')\n",
        "\n",
        "    # Silhouette Score\n",
        "    axes[0, 1].plot(cluster_range_metrics, silhouette_scores[1:], marker='o', color='green')\n",
        "    axes[0, 1].set_title('Silhouette Score')\n",
        "    axes[0, 1].set_xlabel('Number of clusters')\n",
        "    axes[0, 1].set_ylabel('Silhouette Score')\n",
        "\n",
        "    # Calinski-Harabasz Score\n",
        "    axes[1, 0].plot(cluster_range_metrics, calinski_scores[1:], marker='o', color='red')\n",
        "    axes[1, 0].set_title('Calinski-Harabasz Score')\n",
        "    axes[1, 0].set_xlabel('Number of clusters')\n",
        "    axes[1, 0].set_ylabel('Calinski-Harabasz Score')\n",
        "\n",
        "    # Davies-Bouldin Score (lower is better)\n",
        "    axes[1, 1].plot(cluster_range_metrics, davies_scores[1:], marker='o', color='purple')\n",
        "    axes[1, 1].set_title('Davies-Bouldin Score')\n",
        "    axes[1, 1].set_xlabel('Number of clusters')\n",
        "    axes[1, 1].set_ylabel('Davies-Bouldin Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Select optimal number of clusters based on maximum silhouette score (skip cluster 1)\n",
        "    best_index = np.nanargmax(silhouette_scores[1:])\n",
        "    optimal_clusters = best_index + 2  # +2 accounts for starting at 2 clusters\n",
        "    print(f\"Optimal number of clusters based on silhouette score: {optimal_clusters}\")\n",
        "\n",
        "    # Fit and return KMeans with the optimal number of clusters\n",
        "    kmeans = KMeans(n_clusters=25, random_state=0).fit(X)\n",
        "    return kmeans\n",
        "\n",
        "# Cluster embeddings\n",
        "kmeans = cluster_embeddings(embeddings, max_clusters=100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare clusters to serological groups\n",
        "all_alleles= embeddings.keys()\n",
        "# Load serological name (name of allele upto first ':')\n",
        "serological_mapping = all_alleles\n",
        "serological_mapping = [allele.split(':')[0] for allele in all_alleles]\n",
        "# Get cluster labels\n",
        "cluster_labels = kmeans.labels_\n",
        "print(cluster_labels)\n",
        "print(all_alleles)\n",
        "print(serological_mapping)\n",
        "\n",
        "# Create a DataFrame with allele, serological group, and cluster label\n",
        "df = pd.DataFrame({'allele': all_alleles, 'serological': serological_mapping, 'cluster': cluster_labels})\n",
        "df = df.sort_values('cluster')\n",
        "\n",
        "# Plot cluster distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "# sort by count\n",
        "sns.countplot(x='cluster', data=df, order=df['cluster'].value_counts().index)\n",
        "plt.title('Cluster Distribution')\n",
        "plt.show()\n",
        "\n",
        "# Plot serological group distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "# labels should be vertical\n",
        "plt.xticks(rotation=90)\n",
        "# sort by count\n",
        "sns.countplot(x='serological', data=df, order=df['serological'].value_counts().index)\n",
        "plt.title('Serological Group Distribution')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fcc3089",
      "metadata": {},
      "outputs": [],
      "source": [
        "# cross tabulation of serological group and cluster\n",
        "ct = pd.crosstab(df['serological'], df['cluster'])\n",
        "\n",
        "# print the crosstab\n",
        "ct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f4874c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# hierarchical clustering\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "# Compute the distance matrix\n",
        "X = np.array(list(embeddings.values()))\n",
        "Z = linkage(X, method='ward')\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(12, 6))\n",
        "dendrogram(Z, labels=list(embeddings.keys()), orientation='top', leaf_rotation=90)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d480c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, confusion_matrix\n",
        "\n",
        "# Example cluster labels from unsupervised clustering and true serological labels\n",
        "cluster_labels = df['cluster'].values\n",
        "true_labels    = df['serological'].values\n",
        "# convert true lables to integers\n",
        "true_labels = pd.Categorical(true_labels).codes\n",
        "\n",
        "# Compute Adjusted Rand Index (ARI)\n",
        "ari = adjusted_rand_score(true_labels, cluster_labels)\n",
        "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
        "\n",
        "# Compute Normalized Mutual Information (NMI)\n",
        "nmi = normalized_mutual_info_score(true_labels, cluster_labels)\n",
        "print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
        "\n",
        "# Create a contingency table (confusion matrix)\n",
        "contingency = confusion_matrix(true_labels, cluster_labels)\n",
        "print(\"Contingency Table:\")\n",
        "print(contingency)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6abe0ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def clustering_accuracy(true_labels, cluster_labels):\n",
        "    \"\"\"\n",
        "    Compute clustering accuracy using the Hungarian algorithm for optimal label mapping.\n",
        "\n",
        "    Parameters:\n",
        "        true_labels (np.ndarray): Ground truth class labels.\n",
        "        cluster_labels (np.ndarray): Predicted cluster labels from unsupervised clustering.\n",
        "\n",
        "    Returns:\n",
        "        float: Clustering accuracy as the proportion of correctly mapped labels.\n",
        "    \n",
        "    Explanation:\n",
        "    ------------\n",
        "    1. A confusion matrix (or contingency table) is computed where each element [i, j]\n",
        "       represents the number of samples with true label i assigned to cluster j.\n",
        "    2. The Hungarian algorithm (a.k.a. linear sum assignment) is applied on the negative\n",
        "       of this matrix to find an assignment that maximizes the number of correctly matched labels.\n",
        "       Since `linear_sum_assignment` minimizes cost, negating the matrix converts our maximization problem.\n",
        "    3. The sum of the values corresponding to the optimal assignment is divided by the total\n",
        "       number of samples to obtain the accuracy.\n",
        "    \"\"\"\n",
        "    # Compute the contingency table between true and predicted labels\n",
        "    cont_matrix = confusion_matrix(true_labels, cluster_labels)\n",
        "    \n",
        "    # Apply the Hungarian algorithm (linear_sum_assignment) on the negated contingency table.\n",
        "    # This finds the optimal assignment that maximizes the number of matches.\n",
        "    row_ind, col_ind = linear_sum_assignment(-cont_matrix)\n",
        "    \n",
        "    # Sum the counts from the contingency table for the optimal assignment\n",
        "    total_correct = cont_matrix[row_ind, col_ind].sum()\n",
        "    \n",
        "    # Calculate the overall clustering accuracy\n",
        "    accuracy = total_correct / np.sum(cont_matrix)\n",
        "    return accuracy\n",
        "\n",
        "# Example usage of the Hungarian algorithm for clustering accuracy:\n",
        "# Example ground truth labels (serological labels)\n",
        "true_labels = df['serological'].values\n",
        "# Example clustering labels (obtained from an unsupervised clustering algorithm)\n",
        "cluster_labels = df['cluster'].values\n",
        "# convert true lables to integers\n",
        "true_labels = pd.Categorical(true_labels).codes\n",
        "\n",
        "# Compute the clustering accuracy using the Hungarian algorithm\n",
        "acc = clustering_accuracy(true_labels, cluster_labels)\n",
        "print(f\"Clustering Accuracy (Optimal Assignment): {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "5210c78f",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv('clustered_alleles.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
